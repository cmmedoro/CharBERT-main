{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are presenting you all the code cells to run in order to reproduce the experiments we did on CharBERT.\n",
        "\n",
        "In particular, we are going to present the work we did starting from pre-training and then performing the Named Entity Recognition task, on a different domain, like Twitter, and on different languages with respect to English, like Spanish."
      ],
      "metadata": {
        "id": "NHrS9Fqcz7tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the environment"
      ],
      "metadata": {
        "id": "4WAQrTQeeNNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -d -r /content/CharBERT-main"
      ],
      "metadata": {
        "id": "TKzN-zeuJD5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvNw6fczWN0"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/cmmedoro/CharBERT-main.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell should be run if you need to reproduce the experiment where we modified the architecture of CharBERT, by changing the way we obtain char embeddings (instead of concatenating the first and last character, we choose to consider the mean and standard deviation of the characters present in the token)."
      ],
      "metadata": {
        "id": "nsMBRhz80nfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --single-branch --branch no_concatenation https://github.com/cmmedoro/CharBERT-main.git"
      ],
      "metadata": {
        "id": "3u_hbRUHilyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "NWHCJ9RiIUE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "zRab7rOv0ER0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "8MPQHqKw0MB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "JJmti4Qz0Rup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Fsekr9Vm_2DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that for reproducing the experiments we did in terms of the model exploration (branch no_concatenation of the Github repository) you can just reproduce the following two cells, \"Pre-train MLM on English Wikipedia (simple version)\" and \"NER\"."
      ],
      "metadata": {
        "id": "JTsyJj-OQiJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-train MLM on English Wikipedia (simple version)"
      ],
      "metadata": {
        "id": "-kjvRelh10vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For pre-training, we chose to use a simplified version of Wikipedia in the English language. We retain a portion of it and divide it into train/val/test."
      ],
      "metadata": {
        "id": "Q8qOeogM2_zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "9sMCSrCL1685"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")"
      ],
      "metadata": {
        "id": "E0qJdP151-TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = dataset['train'][:500]['text']\n",
        "eval_text = dataset['train'][500:650]['text']\n",
        "test_text = dataset['train'][650:800]['text']"
      ],
      "metadata": {
        "id": "EXcigjP62QeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
        "text = ''\n",
        "for el in train_text:\n",
        "  text += el\n",
        "with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "n0CTv1bF2s2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in eval_text:\n",
        "  text += el\n",
        "with open(\"/content/data/eval.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "WhtW-0hB23CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in test_text:\n",
        "  text += el\n",
        "with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "OFC_XJUp221P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two cells are to be run if the pre-training needs to happen from a previous checkpoint of the model."
      ],
      "metadata": {
        "id": "fcT9Q21o_y4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "FHUL9YlS_x3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/mlm_training_3epochs.zip -d /content/ckpt"
      ],
      "metadata": {
        "id": "LDeaz5tA__lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-train MLM for 3 epochs:"
      ],
      "metadata": {
        "id": "ayrEjwUNAQkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR= \"/content/data\"\n",
        "MODEL_DIR=\"/content/ckpt/model_pretrained\" # Here you need to insert the path to the model checkpoint downloaded\n",
        "# Note that if you are passing a checkpoint, you need to modify \"--model_name_or_path bert-base-cased\" in \"--model_name_or_path $MODEL_DIR\"\n",
        "OUTPUT_DIR=\"/content/output/mlm\"\n",
        "!python3 /content/CharBERT-main/run_lm_finetuning.py --model_type bert --model_name_or_path bert-base-cased --do_train --do_eval --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --term_vocab /content/CharBERT-main/data/dict/term_vocab --train_data_file $DATA_DIR/train.txt --eval_data_file $DATA_DIR/eval.txt --learning_rate 3e-5 --num_train_epochs 1 --mlm_probability 0.10 --input_nraws 1000 --per_gpu_train_batch_size 4 --per_gpu_eval_batch_size 4 --save_steps 10000 --block_size 384 --overwrite_output_dir --mlm --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "hVZt_-fVAZmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r mlm_epoch_3.zip /content/$/content/output/mlm"
      ],
      "metadata": {
        "id": "MdNudu71MQFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this code was also used to further train the model for other 3 epochs, so for a total of 6 epochs, when fine-tuning the model on Twitter data, for following experiments."
      ],
      "metadata": {
        "id": "6kajkQP0BRuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER"
      ],
      "metadata": {
        "id": "DwNOI5pz313J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On CoNLL-2003."
      ],
      "metadata": {
        "id": "Yu46o_ZeC_Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "w4Ll4Zv51U-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/conll2003.zip -d /content/CharBERT-main/data # Download CoNLL-2003"
      ],
      "metadata": {
        "id": "7VqJE3ru3GqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/mlm_training_3epochs.zip -d /content/ckpt # Checkpoint of model"
      ],
      "metadata": {
        "id": "KOA5uTnIQFkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "DATA_DIR= \"/content/CharBERT-main/data/conll2003\" # Path to data\n",
        "MODEL_DIR= \"/content/kaggle/working/$/kaggle/working/mlm\" # Path to checkpoint of model\n",
        "OUTPUT_DIR=\"/content/output/ner\"\n",
        "!python3 /content/CharBERT-main/run_ner.py --model_type bert --data_dir $DATA_DIR --model_name_or_path $MODEL_DIR --do_train --do_predict --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --learning_rate 3e-5 --num_train_epochs 1  --per_gpu_train_batch_size 4 --overwrite_output_dir --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "CSWXE-Fvtrhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/$/content/output/ner/checkpoint-1150"
      ],
      "metadata": {
        "id": "QjeshWr_fx20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ner_conll_epoch3.zip /content/$/content/output/mlm"
      ],
      "metadata": {
        "id": "pM7vAE7_LoYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning on Twitter data"
      ],
      "metadata": {
        "id": "U4ZMAJm-Nns1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to perform domain adaptation of CharBERT on social media data coming from Twitter. The idea is to take a dataset with tweets, which more or less has the same dimenson as the one of Wikipedia used for pre-train, and further fine-tune CharBERT (the one obtained after 3 epochs of training on English Wikipedia) for 3 additional epochs. Note that for the experiments in this setting we also trained for 3 additional epochs the model on English Wikipedia, to have comparable models to then perform the downstream task."
      ],
      "metadata": {
        "id": "7GHeyhH1OR0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "sp64tw85A88N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/twitter_cikm_2010.zip -d /content/twitter_data"
      ],
      "metadata": {
        "id": "16x38l5d2EuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/twitter_data/training_set_tweets.txt', sep='\\t', on_bad_lines = 'skip', header = None)"
      ],
      "metadata": {
        "id": "PyWVArOpHidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df[2]\n",
        "tweets = []\n",
        "for text in texts.values:\n",
        "  if isinstance(text, str):\n",
        "    tweets.append(text)"
      ],
      "metadata": {
        "id": "Ljolc-FZQukW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_lines_with_only_numbers(tweets):\n",
        "    filtered_lines = [tw for tw in tweets if not re.match(r'^\\d+$', tw)]\n",
        "    return filtered_lines\n",
        "filtered = remove_lines_with_only_numbers(tweets)\n",
        "filt = pd.DataFrame(filtered)"
      ],
      "metadata": {
        "id": "GiXMKK_U1hhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = filt[:22000]\n",
        "eval = filt[22000:29000]\n",
        "test = filt[29000:36000]"
      ],
      "metadata": {
        "id": "vywD5GH53fwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in train.values:\n",
        "  text += el[0] + '\\n'\n",
        "with open(\"/content/twitter_data/train.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "3QZgMkqE4_YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in eval.values:\n",
        "  text += el[0] + '\\n'\n",
        "with open(\"/content/twitter_data/eval.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "QY7YqKfc45zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in test.values:\n",
        "  text += el[0] + '\\n'\n",
        "with open(\"/content/twitter_data/test.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "ZgS_BGS_EC6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model checkpoint\n",
        "!unzip -q /content/drive/MyDrive/NLP/mlm_training_3epochs.zip -d /content/ckpt"
      ],
      "metadata": {
        "id": "dncJDd88U6pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING SU TWEETS\n",
        "DATA_DIR= \"/content/twitter_data\"\n",
        "MODEL_DIR=\"/content/ckpt/content/$/content/output/mlm\"\n",
        "OUTPUT_DIR=\"/content/output/mlm\"\n",
        "!python3 /content/CharBERT-main/run_lm_finetuning.py --model_type bert --model_name_or_path $MODEL_DIR --do_train --do_eval --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --term_vocab /content/CharBERT-main/data/dict/term_vocab --train_data_file $DATA_DIR/train_tw.txt --eval_data_file $DATA_DIR/eval_tw.txt --learning_rate 3e-5 --num_train_epochs 1 --mlm_probability 0.10 --input_nraws 1000 --per_gpu_train_batch_size 4 --per_gpu_eval_batch_size 4 --save_steps 10000 --block_size 384 --overwrite_output_dir --mlm --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "DDohEXot6AVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fine_tuning_twitter_epoch6.zip /content/$/content/output/mlm"
      ],
      "metadata": {
        "id": "3VaV8ieC-24C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER Twitter"
      ],
      "metadata": {
        "id": "BYSFMZz60CX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to assess the performances of the model fine-tuned on Twitter data on a NER dataset taken from Twitter. We are going to compare the performances with the model trained on English Wikipedia."
      ],
      "metadata": {
        "id": "mcr07dV3Q8Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "zFnZpAncXUut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"tner/tweetner7\")"
      ],
      "metadata": {
        "id": "7QRK3AaX0E_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to perform mapping of the labels as CharBERT is based on four main entities, while the current dataset has seven entities."
      ],
      "metadata": {
        "id": "XHhYeIoGRL6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {0:'B-ORG',1:'B-MISC',2:'B-MISC',3:'B-ORG',4:'B-LOC',5:'B-PER',6:'B-MISC',7:'I-ORG',8:'I-MISC',9:'I-MISC',10:'I-ORG',11:'I-LOC',12:'I-PER',13:'I-MISC',14:'O'}"
      ],
      "metadata": {
        "id": "_ys9oDQ75uYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use \"train_all\", \"test_2021\" and \"validation_2021\"."
      ],
      "metadata": {
        "id": "79bFmcm6XuKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = ''\n",
        "for batch_list in dataset['train_all']:\n",
        "  for i in range(len(batch_list['tokens'])):\n",
        "    train += batch_list['tokens'][i] +' '+label_mapping[batch_list['tags'][i]]+'\\n'\n",
        "  train +='\\n'"
      ],
      "metadata": {
        "id": "7_lMkMmuX5XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation = ''\n",
        "for batch_list in dataset['validation_2021']:\n",
        "  for i in range(len(batch_list['tokens'])):\n",
        "    validation += batch_list['tokens'][i] +' '+label_mapping[batch_list['tags'][i]]+'\\n'\n",
        "  validation += '\\n'"
      ],
      "metadata": {
        "id": "SSC__bq5X9XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for batch_list in dataset['test_2021']:\n",
        "  for i in range(len(batch_list['tokens'])):\n",
        "    text += batch_list['tokens'][i] +' '+label_mapping[batch_list['tags'][i]]+'\\n'\n",
        "  text += '\\n'"
      ],
      "metadata": {
        "id": "sU_zpU7ODcuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
        "with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(train)"
      ],
      "metadata": {
        "id": "605R-BYtYF7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/data/validation.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(validation)"
      ],
      "metadata": {
        "id": "etc8nwKMYFj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "c0TGY2Ui_X3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "79jEuatIXE0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP_PROVE/mlm_epoch6.zip -d /content/ckpt"
      ],
      "metadata": {
        "id": "odDRFwzuXFyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "DATA_DIR= \"/content/data\"\n",
        "MODEL_DIR= \"/content/ckpt/content/$/content/output/mlm\"\n",
        "OUTPUT_DIR=\"/content/output/ner\"\n",
        "!python3 /content/CharBERT-main/run_ner.py  --model_type bert --data_dir $DATA_DIR --model_name_or_path $MODEL_DIR --do_train --do_predict --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --learning_rate 3e-5 --num_train_epochs 3 --save_steps 500 --per_gpu_train_batch_size 4 --overwrite_output_dir --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "9yxF4xM3WUzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/$/content/output/ner/checkpoint-4500"
      ],
      "metadata": {
        "id": "uyks8g6dU_32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r tner_our_wikipedia_epoch3.zip /content/$/content/output/ner"
      ],
      "metadata": {
        "id": "ebsp-QCMUzYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual extension"
      ],
      "metadata": {
        "id": "rScG-eY9fOdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to train a model on a dataset comprising both English and Italian Wikipedia."
      ],
      "metadata": {
        "id": "mPIyq8UxSEvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "cHhAVTsWBp39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_it = load_dataset(\"wikipedia\", \"20220301.it\")"
      ],
      "metadata": {
        "id": "il3NICA6_t9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_it = dataset_it['train'][:200]['text']"
      ],
      "metadata": {
        "id": "8nXF4EqWo9ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_text_it = dataset_it['train'][200:220]['text']\n",
        "test_text_it = dataset_it['train'][220:240]['text']"
      ],
      "metadata": {
        "id": "sUI6Lqg4pGYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_en = load_dataset(\"wikipedia\", \"20220301.en\")"
      ],
      "metadata": {
        "id": "T5v7nM8vJJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_en = dataset_en['train'][:30]['text']\n",
        "eval_text_en = dataset_en['train'][30:37]['text']\n",
        "test_text_en = dataset_en['train'][37:45]['text']"
      ],
      "metadata": {
        "id": "dZAIO4M27dZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
        "text = ''\n",
        "for el in train_text_it:\n",
        "  text += el\n",
        "for el in train_text_en:\n",
        "  text += el\n",
        "with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "OLdSxlLHbNqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in eval_text_it:\n",
        "  text += el\n",
        "for el in eval_text_en:\n",
        "  text += el\n",
        "with open(\"/content/data/eval.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "GDXE7RKRH6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for el in test_text_it:\n",
        "  text += el\n",
        "for el in test_text_en:\n",
        "  text += el\n",
        "with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "KKtLFYcLIAmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "V6KrV2h93-02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/multilingual_it_eng_epoch3.zip -d /content/ckpt"
      ],
      "metadata": {
        "id": "o6A704AR4SRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR= \"/content/data\"\n",
        "MODEL_DIR=\"/content/content/model_pretrained\"\n",
        "OUTPUT_DIR=\"/content/output/mlm\"\n",
        "!python3 /content/CharBERT-main/run_lm_finetuning.py --model_type bert --model_name_or_path bert-base-multilingual-cased --do_train --do_eval --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --term_vocab /content/CharBERT-main/data/dict/term_vocab --train_data_file $DATA_DIR/train.txt --eval_data_file $DATA_DIR/eval.txt --learning_rate 3e-5 --num_train_epochs 1 --mlm_probability 0.10 --input_nraws 1000 --per_gpu_train_batch_size 4 --per_gpu_eval_batch_size 4 --save_steps 10000 --block_size 384 --overwrite_output_dir --mlm --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "YlaZrUMZLE1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r multilingual_epoch1.zip /content/$/content/output/mlm"
      ],
      "metadata": {
        "id": "BrfRKaR1Oo3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER multilingual"
      ],
      "metadata": {
        "id": "jzTtqAPhLZy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to assess the performances of CharBERT pre-trained on English and Italian Wikipedia data on the downstream task of NER. We are going to assess the performance separately on each language."
      ],
      "metadata": {
        "id": "gya55YphTgr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Babelscape/wikineural\")"
      ],
      "metadata": {
        "id": "pstTJTHKLhkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {0:'O', 1: 'B-PER', 2:'I-PER', 3 : 'B-ORG', 4:'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8:'I-MISC'}"
      ],
      "metadata": {
        "id": "W2FmuQLBL-gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Italian Wikipedia NER:"
      ],
      "metadata": {
        "id": "usVWGXHx2CJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['train_it'][:3000]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        train += batch_list[i] +' '+label_mapping[dataset['train_it'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    train +='\\n'"
      ],
      "metadata": {
        "id": "hmO3ByNjOx_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['val_fr'][:1900]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        validation += batch_list[i] +' '+label_mapping[dataset['val_fr'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    validation +='\\n'"
      ],
      "metadata": {
        "id": "VTscaVgH2ZyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['test_fr'][:1900]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        test += batch_list[i] +' '+label_mapping[dataset['test_fr'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    test +='\\n'"
      ],
      "metadata": {
        "id": "0ha5FJ2n2dlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "English Wikipedia NER:"
      ],
      "metadata": {
        "id": "surhJrxI2p52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['train_en'][:3500]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        train += batch_list[i] +' '+label_mapping[dataset['train_en'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    train +='\\n'"
      ],
      "metadata": {
        "id": "yhWi2pVZ2UKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['val_en'][:2000]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        validation += batch_list[i] +' '+label_mapping[dataset['val_en'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    validation +='\\n'"
      ],
      "metadata": {
        "id": "x1C4yh0eO9E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = ''\n",
        "for phrase_number, batch_list in enumerate(dataset['test_en'][:2000]['tokens']):\n",
        "    for i in range(len(batch_list)):\n",
        "        test += batch_list[i] +' '+label_mapping[dataset['test_en'][phrase_number]['ner_tags'][i]]+'\\n'\n",
        "    test +='\\n'"
      ],
      "metadata": {
        "id": "63Wiv_bkPhRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the train/validation/test splits into file txt."
      ],
      "metadata": {
        "id": "r2K4KAhu2uWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
        "with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(train)"
      ],
      "metadata": {
        "id": "l7jKGikcP87Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/data/validation.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(validation)"
      ],
      "metadata": {
        "id": "eVL_ecbeP87Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
        "  f.write(test)"
      ],
      "metadata": {
        "id": "L3mCstHsP87a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Nvb9MTJMP87a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/NLP/multilingual_it_eng_epoch3.zip -d /content/ckpt"
      ],
      "metadata": {
        "id": "fF6GVkuvP87a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NER MULTILINGUAL\n",
        "DATA_DIR= \"/content/data\"\n",
        "MODEL_DIR= \"/content/ckpt/content/$/content/output/mlm\"\n",
        "OUTPUT_DIR=\"/content/output/ner\"\n",
        "!python3 /content/CharBERT-main/run_ner.py  --model_type bert --data_dir $DATA_DIR --model_name_or_path $MODEL_DIR --do_train --do_predict --char_vocab /content/CharBERT-main/data/dict/bert_char_vocab --learning_rate 3e-5 --num_train_epochs 3 --save_steps 500 --per_gpu_train_batch_size 4 --overwrite_output_dir --output_dir ${OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "y-xHyI4QP87a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/$/content/output/ner/checkpoint-4500"
      ],
      "metadata": {
        "id": "K5ZTrbitlaDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r multilingual_ner_epoch1.zip /content/$/content/output/ner"
      ],
      "metadata": {
        "id": "GGYTNzPjljqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}